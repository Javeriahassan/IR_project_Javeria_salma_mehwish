{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound as s\n",
    "s.MessageBeep()\n",
    "s.Beep(1300,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en')\n",
    "\n",
    "from spacy.en import English\n",
    "parser = English()\n",
    "\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "#     text.translate(dict.fromkeys(string.punctuation))\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.orth_.startswith('@'):\n",
    "            continue\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "s.Beep(1300,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Faaiza\n",
      "[nltk_data]     Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs dog dog\n",
      "ran run ran\n",
      "discouraged discourage discouraged\n"
     ]
    }
   ],
   "source": [
    "for w in ['dogs', 'ran', 'discouraged']:\n",
    "    print(w, get_lemma(w), get_lemma2(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Faaiza\n",
      "[nltk_data]     Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Cleaning\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "text_data = []\n",
    "short_pos = open(\"dataset/combinedtweet.txt\",\"r\",encoding=\"ANSI\").read()\n",
    "tweet = short_pos.split('\\n')\n",
    "document = []\n",
    "for t in tweet:\n",
    "      item = t.split('|')\n",
    "      if len(item)>=3:\n",
    "          clean = re.sub(r\"http\\S+\", \"\", item[2])\n",
    "          document.append(clean)\n",
    "print (\"Done Cleaning\")\n",
    "for line in document:\n",
    "    tokens = prepare_text_for_lda(line)\n",
    "#     if random.random() > .99:\n",
    "#     print(tokens)\n",
    "    text_data.append(tokens)\n",
    "print (\"Done...\")\n",
    "s.Beep(1300,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59817"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 4\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=25)\n",
    "ldamodel.save('model4.gensim')\n",
    "s.Beep(1300,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.078*\"ebola\" + 0.025*\"patient\" + 0.017*\"hospital\" + 0.013*\"doctor\" + 0.010*\"treatment\" + 0.009*\"state\" + 0.008*\"africa\" + 0.008*\"first\" + 0.008*\"healthy\" + 0.007*\"fight\"')\n",
      "(1, '0.039*\"study\" + 0.016*\"heart\" + 0.012*\"find\" + 0.010*\"story\" + 0.010*\"brain\" + 0.009*\"medicare\" + 0.009*\"disease\" + 0.008*\"american\" + 0.008*\"exercise\" + 0.008*\"diabetes\"')\n",
      "(2, '0.069*\"health\" + 0.013*\"people\" + 0.010*\"insurance\" + 0.008*\"better\" + 0.008*\"question\" + 0.007*\"obama\" + 0.007*\"expert\" + 0.007*\"recipe\" + 0.006*\"change\" + 0.006*\"exchange\"')\n",
      "(3, '0.030*\"cancer\" + 0.013*\"pharma\" + 0.011*\"report\" + 0.011*\"drug\" + 0.011*\"medical\" + 0.009*\"million\" + 0.009*\"obamacare\" + 0.008*\"weight\" + 0.007*\"death\" + 0.007*\"breast\"')\n"
     ]
    }
   ],
   "source": [
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(new_tweet):\n",
    "    new_tweet = prepare_text_for_lda(new_tweet)\n",
    "    new_tweet_bow = dictionary.doc2bow(new_tweet)\n",
    "#     print(new_doc_bow)\n",
    "    pred =ldamodel.get_document_topics(new_tweet_bow)\n",
    "#     print (pred)\n",
    "    max=pred[0][1]\n",
    "    label=0\n",
    "    for i in range(1,4):\n",
    "        if (pred[i][1]>max):\n",
    "            max= pred[i][1]\n",
    "            label=i\n",
    "    return label\n",
    "#     print (label)\n",
    "#     print('Label is %d with proabbility of %f' % (label,max))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model4.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:387: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  topic_term_dists = topic_term_dists.ix[topic_order]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8891/    [Ctrl-C to exit]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [27/May/2018 14:24:03] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/May/2018 14:24:04] \"GET /LDAvis.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/May/2018 14:24:04] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [27/May/2018 14:24:04] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.show(lda_display)\n",
    "s.Beep(1300,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda3 = gensim.models.ldamodel.LdaModel.load('model3.gensim')\n",
    "lda_display3 = pyLDAvis.gensim.prepare(lda3, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Faaiza\n",
      "[nltk_data]     Iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')\n",
    "documents_final = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "              for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = open(\"dataset/health_tweets_labeled.csv\", \"a\") \n",
    "\n",
    "columnTitleRow = \"tweet, class\\n\"\n",
    "csv.write(columnTitleRow)\n",
    "\n",
    "for i in range(13618,len(text_data)):\n",
    "    tweet = text_data[i]\n",
    "    tweet_str = ' '.join(str(e) for e in tweet)\n",
    "#     re.sub(r'[^\\x00-\\x7F]+',' ', tweet_str)\n",
    "    row = tweet_str.replace(\",\",\"\") + \",\" + str(predict_class(tweet_str)) + \"\\n\"\n",
    "    csv.write(row)\n",
    "csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exercise', 'outside', 'simple', 'ã¢â‚¬â€\\x9d']\n"
     ]
    }
   ],
   "source": [
    "# print(text_data[13617])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
